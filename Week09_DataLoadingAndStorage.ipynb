{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9\n",
    "# Data Loading and Storage\n",
    "\n",
    "Accessing data is a necessary first step for most data science projects. From this chapter we will learn:\n",
    "- Reading and writing data in text format (.txt, .csv, .json)\n",
    "- Reading data from webpages (web scrapping)\n",
    "- Reading and writing data in binary format (.pickle, .feather, .h5)\n",
    "- Interacting with databases\n",
    "\n",
    "Reading:\n",
    "- Textbook, Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Reading and Writing Data in Text Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's create a data frame first\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "values = np.array([\n",
    "    [100, 80, 95, 'A'],\n",
    "    [55, 60, 45, 'F'],\n",
    "    [70, 75, 90, 'A'],\n",
    "    [75, 70, 60, 'D'],\n",
    "    [60, 73, 75, 'C'],\n",
    "    [72, 63, -1, 'NA']\n",
    "])\n",
    "df = pd.DataFrame(values,\n",
    "                   columns=['Midterm', 'Project', 'Final', 'LetterGrade'],\n",
    "                   index=['Alex', 'Bob', 'Chris', 'Doug', 'Eva', \"Frank\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a csv file using .to_csv()\n",
    "import os\n",
    "print('Does path \"Data/temp/\" exist?', os.path.exists(\"Data/temp/\"))\n",
    "\n",
    "if not os.path.exists(\"Data/temp\"):\n",
    "    os.mkdir(\"Data/temp\")\n",
    "    print('File path \"Data/temp\" created.')\n",
    "\n",
    "df.to_csv(\"Data/temp/grades.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "df2 = pd.read_csv(\"Data/temp/grades.csv\", sep=\",\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only the first 3 rows\n",
    "df3 = pd.read_csv(\"Data/temp/grades.csv\", nrows=3)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file, skipping row 2 and 4\n",
    "df4 = pd.read_csv(\"Data/temp/grades.csv\", skiprows=[2, 4])\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column headers from the csv file, then load it\n",
    "# df5 = pd.read_csv(\"Data/temp/grades.csv\", header=None, names=['Name', 'Midterm', 'Project', 'Final', 'LetterGrade'])\n",
    "df5 = pd.read_csv(\"Data/temp/grades.csv\", names=[1, 2, 3, 4, 5], skiprows=[0])\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set first column as index\n",
    "df6 = pd.read_csv(\"Data/temp/grades.csv\", index_col=0)\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify -1 as NaN\n",
    "df7 = pd.read_csv(\"Data/temp/grades.csv\", na_values=[-1, 63])\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load txt file with values separated by spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/temp/values.txt\", 'w') as file:\n",
    "    file.write(\"Index Category     Value\\n\")\n",
    "    file.write(\"1            A      2.92\\n\")\n",
    "    file.write(\"2            B     12.14\\n\")\n",
    "    file.write(\"3            C    123.56\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although read_csv() is still applicable, setting delimiter to a single space will create errors\n",
    "df = pd.read_csv(\"Data/temp/values.txt\", sep=\" \")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/temp/values.txt\", sep=\"\\s+\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load JSON files\n",
    "\n",
    "**JavaScript Object Notation (JSON)** is a popular file format to storing unstructured data because it is easy for both human and computer to understand.\n",
    "- Its structure is very similar to Python dictionary\n",
    "- Load a json file with json.loads()\n",
    "- Writes to a json file with json.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = \"\"\"\n",
    "{\"name\": \"Wes\",\n",
    " \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
    " \"pet\": null,\n",
    " \"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n",
    "              {\"name\": \"Katie\", \"age\": 38,\n",
    "               \"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "result = json.loads(obj)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A JSON object is represented as a python dictionary\n",
    "?result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asjson = json.dumps(result) # Convert back to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use json.dump(object, file) to write the content to file\n",
    "with open(\"Data/temp/People.json\", 'w') as file:\n",
    "    json.dump(result, file)\n",
    "    \n",
    "# The with statement is equivalent to the following:\n",
    "# file = open(\"Data/temp/People.json\", 'w')\n",
    "# json.dump(result, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load from People.json\n",
    "with open(\"Data/temp/People.json\", \"r\") as file:\n",
    "    people = json.load(file)\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the content as a data frame\n",
    "siblings = pd.DataFrame(result['siblings'], columns=['name', 'age', 'pets'])\n",
    "siblings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Web Scrapping\n",
    "When performing data science tasks, it's common to want to use data found on the internet. You'll usually be able to access the data in csv format, or via an Application Programming Interface (API). However, there are times when the data you want can only be accessed as part of a web page. In cases like this, you'll want to use a technique called **web scraping** to get the data from the web page into a format you can work with in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a webpage\n",
    "import requests\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "page #2** status code usually means successful download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what is downloaded\n",
    "print(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **beautifulsoup** library to extract useful information from the html script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the children attribute to select all the top-level tags\n",
    "list(soup.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# type of each children\n",
    "print([type(item) for item in list(soup.children)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the html tag and its children by taking the third item in the list:\n",
    "html = list(soup.children)[2]\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n'.join([str(idx) + ':\\n' + str(item) \\\n",
    "                 for idx, item in enumerate(list(html.children))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(list(html.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([type(item) for item in list(html.children)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = list(html.children)[3]\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(body.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(body.children)[1]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: find the name \"Brian J. Murphy\" for Dr. Murphy's website.\n",
    "page = requests.get(\"http://comet.lehman.cuny.edu/bmurphy/\")\n",
    "soup2 = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup2.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1_children = list(soup2.children)\n",
    "print(len(level1_children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level2_children = list(level1_children[0])\n",
    "print(len(level2_children))\n",
    "print(level2_children[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level3_children = list(level2_children[3])\n",
    "print(len(level3_children))\n",
    "print(level3_children[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level4_children = list(level3_children[13])\n",
    "print(len(level4_children))\n",
    "print(level4_children[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = level4_children[1]\n",
    "print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex: Extract all the button labels\n",
    "\n",
    "index_list = [3, 5, 7, 9, 11]\n",
    "for i in index_list:\n",
    "    button = list(list(list(soup2.children)[0].children)[3])[i]\n",
    "    print(button['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FInding all instances of a tag at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup.find_all('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buttons = soup.find_all('input')\n",
    "for button in all_buttons:\n",
    "    print(button['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first instance of a tag\n",
    "soup.find('input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching for tags by class and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at another webpage with classes and id's\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all tags of a class\n",
    "soup.find_all(class_=\"first-item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(id=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the weather data\n",
    "1. Open the [weather forecast page](https://forecast.weather.gov/MapClick.php?lat=40.7146&lon=-74.0071#.Xbc5aXVKhhE)\n",
    "2. Display the source code (On Chrome use \"Developer Tools\")\n",
    "3. Identify the item containing data (On Chrome right click the values and select \"Inspect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://forecast.weather.gov/MapClick.php?lat=40.7146&lon=-74.0071#.Xbc5aXVKhhE\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "seven_day = soup.find(id=\"seven-day-forecast\")\n",
    "# print(len(seven_day))\n",
    "# print(seven_day)\n",
    "forecast_items = seven_day.find_all(class_=\"tombstone-container\")\n",
    "# print(len(forecast_items))\n",
    "# print(forecast_items)\n",
    "tonight = forecast_items[0]\n",
    "print(tonight.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find today's weather\n",
    "\n",
    "items = soup.find_all(class_=\"myforecast-current-lrg\")\n",
    "items[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = soup.find_all(class_=\"period-name\") \n",
    "# This statement creates a list of temperature labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of p objects to a list of strings\n",
    "days = []\n",
    "for obj in names:\n",
    "#     print(obj.get_text())\n",
    "    days.append(obj.get_text())\n",
    "print(days)\n",
    "\n",
    "days = [obj.get_text() for obj in names]\n",
    "print(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = soup.find_all(class_=\"temp\")\n",
    "# Retrieve all the temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from each temperature object\n",
    "temperatures = [obj.get_text() for obj in temps]\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([days, temperatures]).T\n",
    "# transpose the array so that each list becomes a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create a data frame with the days and the temperatures\n",
    "df = pd.DataFrame(data, columns=[\"Day\", \"Temperature\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find weather forecast for the week\n",
    "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
    "periods = [pt.get_text() for pt in period_tags]\n",
    "periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find short descriptions and long descriptions for the week\n",
    "short_desc_tags = seven_day.select(\".tombstone-container .short-desc\")\n",
    "short_descs = [pt.get_text() for obj in short_desc_tags]\n",
    "print(short_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_desc_tags = seven_day.select(\".tombstone-container .forecast-icon\")\n",
    "descs = [obj['title'] for obj in long_desc_tags]\n",
    "print(long_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tags = seven_day.select(\".tombstone-container .temp\")\n",
    "temps = [obj.get_text() for obj in temp_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weather data as a data frame\n",
    "import pandas as pd\n",
    "weather = pd.DataFrame({\n",
    "    \"period\": periods,\n",
    "    \"short_desc\": short_descs,\n",
    "    \"temp\": temps,\n",
    "    \"desc\":descs\n",
    "})\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract numeric temperature\n",
    "temp_nums = weather[\"temp\"].str.extract(\"(?P<temp_num>\\d+)\", expand=False)\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html\n",
    "weather[\"temp_num\"] = temp_nums.astype('int')\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify day temperature from night temperature\n",
    "is_night = weather[\"temp\"].str.contains(\"Low\")\n",
    "weather[\"is_night\"] = is_night\n",
    "is_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[is_night]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new headlines from New York Times?\n",
    "# Get current stock prices?\n",
    "# Monitor alarms?;\n",
    "# Download files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Binary File Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. pickle\n",
    "The `pickle` module implements binary protocols for serializing and de-serializing a Python object structure. Only Python can properly read and write pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a data frame first\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "values = np.array([\n",
    "    [100, 80, 95, 'A'],\n",
    "    [55, 60, 45, 'F'],\n",
    "    [70, 75, 90, 'A'],\n",
    "    [75, 70, 60, 'D'],\n",
    "    [60, 73, 75, 'C'],\n",
    "    [72, 63, -1, 'NA']\n",
    "])\n",
    "df = pd.DataFrame(values,\n",
    "                   columns=['Midterm', 'Project', 'Final', 'LetterGrade'],\n",
    "                   index=['Alex', 'Bob', 'Chris', 'Doug', 'Eva', \"Frank\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a .pickle file\n",
    "df.to_pickle(\"data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle file\n",
    "df_pickle = pd.read_pickle(\"data.pickle\")\n",
    "df_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pickle file can contain multiple objects.\n",
    "import pickle\n",
    "a = 5\n",
    "b = ['a', 'b', 'c']\n",
    "with open('temp.pickle', 'wb') as file:\n",
    "    pickle.dump(a, file)\n",
    "    pickle.dump(b, file)\n",
    "    pickle.dump(df_pickle, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.pickle', 'rb') as file:\n",
    "    a = pickle.load(file)\n",
    "    b = pickle.load(file)\n",
    "    df_pickle = pickle.load(file)\n",
    "    \n",
    "print(a)\n",
    "print(b)\n",
    "df_pickle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HDF5\n",
    "The \"HDF\" stands for \"hierarchical data format\". HDF5 can be a good choice for working with very large datasets that don't fit into memory, as you can efficiently read and write small sections of large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Col1': np.random.randn(100),\n",
    "    'Col2': np.random.randn(100)\n",
    "})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PyTable package may require update\n",
    "!pip3 install --upgrade tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('data.h5', 'obj1', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdf5 = pd.read_hdf('data.h5', 'obj1', where=['index < 3'])\n",
    "df_hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Interacting with Databases\n",
    "In a business setting, most data may not be stored in text or binary files. SQL-based relational databases (such as mySQL) are in wide use.\n",
    "\n",
    "Python has sqlite3 package to interact with databases, and Pandas has some functions to simplify the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQLite database\n",
    "import sqlite3\n",
    "query = \"\"\"\n",
    "CREATE TABLE tb\n",
    "(a VARCHAR(20), b VARCHAR(20),\n",
    " c REAL,        d INTEGER\n",
    ");\"\"\"\n",
    "con = sqlite3.connect('data.sqlite')\n",
    "con.execute(query)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "# DROP TABLE test\n",
    "# \"\"\"\n",
    "# con.execute(query)\n",
    "# con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a few rows of data\n",
    "data = [('Atlanta', 'Georgia', 1.25, 6),\n",
    "        ('Tallahassee', 'Florida', 2.6, 3),\n",
    "        ('Sacramento', 'California', 1.7, 5)]\n",
    "stmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\n",
    "con.executemany(stmt, data)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data\n",
    "cursor = con.execute('select * from test')\n",
    "rows = cursor.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve columns names\n",
    "cursor.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas data frame\n",
    "columns = [x[0] for x in cursor.description]\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
